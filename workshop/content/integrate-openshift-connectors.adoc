:icons: font

== Integrate an application with cloud services using OpenShift Connectors

=== Introduction

The Globex Coolstuff application allows users to order products by adding selected items to their shopping cart, and checking out the shopping cart. When placing the order, an order payload in JSON format is sent to the REST endpoint of the Order Placement service. This service transforms the payload into a Kafka message which is sent to a topic on the Kafka broker.

=== Workshop Activities

In this part of the workshop you will connect the Order Placement service to the OpenShift Streams for Apache Kafka instance using Service Binding.
You will then leverage Red Hat OpenShift Connectors to connect the Streams for Apache Kafka instance to a third party cloud service to e.g. process or analyze the orders. The external cloud service itself (which typically would be something like AWS Kinesis, AWS S3, Azure Functions, Google Cloud Functions) is outside the scope of this workshop, so instead you will use a simple hosted HTTP endpoint to illustrate the integration with Streams for Apache Kafka instance.

=== Step-By-Step Instructions

==== Bind the Order Placement application to Streams for Apache Kafka

The first steps consist of creating a topic for the orders in the Streams for Apache Kafka instance, and bind the Order Placement application to Streams for Apache Kafka.

These steps are very similar to what you did in previous sections of the lab, so the steps won't be repeated in great detail here. Please refer to previous sections if you need more details.

===== [underline]#*Create a Kafka Topic in OpenShift Streams for Apache Kafka*#

. On the https://console.redhat.com[console.redhat.com] landing page, select *Application and Data Services* from the menu on the left. On the Application Services landing page, select *Streams for Apache Kafka â†’ Kafka Instances*.

. In the *Kafka Instances* page of the web console, click the name of the Kafka instance (`globex`) that you want to add a topic to.

. Select the *Topics* tab, click *Create topic*, and follow the guided steps to define the topic details. Click *Next* to complete each step and click *Finish* to complete the setup.
* *Topic name*: Enter `globex.orders`.
* *Partitions*: Keep the default value at `1`. 
* *Message retention*: Keep default values.
* *Replicas*: Keep default values

===== [underline]#*Bind the Order Placement application to Streams for Apache Kafka*#

. In a browser window, navigate to the console of your OpenShift cluster at %openshift_cluster_console%. Open the *Developer* perspective.
. Open the *Topology* view of the OpenShift console in the *globex-%user%* namespace.
. Hover over the *order-placement* deployment, and grab the arrow that appears. Drag the arrow to the *KafkaConnection* icon. When reaching the KafkaConnection icon, a text box `Create Service Binding` appears. Release the arrow. Click *Create* in the *Create Service Binding* pop-up window. The order PLacement deployment and the KafkaConnection icon are now connected with a solid black arrow.
. Scale the *order-placement* deployment to 1 replica. You can do so by clicking on the *order-placement* deployment, and in the details window select the *Details* tab, and click the arrow next to the circle to scale the deployment.
. Check the logs of the order-placement pod, and notice that the pod successfully connects to the Kafka broker instance.
+
----
exec java -Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -XX:+ExitOnOutOfMemoryError -cp . -jar /deployments/quarkus-run.jar
__  ____  __  _____   ___  __ ____  ______ 
 --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ 
 -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\ \   
--\___\_\____/_/ |_/_/|_/_/|_|\____/___/   
2022-08-03 15:18:44,217 WARN  [io.qua.net.run.NettyRecorder] (Thread-0) Netty DefaultChannelId initialization (with io.netty.machineId system property set to 00:57:48:9d:c7:a6:32:56) took more than a second
2022-08-03 15:18:51,918 INFO  [org.apa.kaf.com.sec.aut.AbstractLogin] (main) Successfully logged in.
2022-08-03 15:18:55,119 INFO  [io.sma.rea.mes.kafka] (main) SRMSG18258: Kafka producer kafka-producer-order-event, connected to Kafka brokers 'globex-cbl--uikhkqj-qi-qdfg.bf2.kafka.rhcloud.com:443', is configured to write records to 'globex.orders'
2022-08-03 15:18:59,517 INFO  [io.quarkus] (main) order-placement-service 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.7.4.Final) started in 27.396s. Listening on: http://0.0.0.0:8080
2022-08-03 15:18:59,517 INFO  [io.quarkus] (main) Profile prod activated. 
2022-08-03 15:18:59,517 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-client, resteasy-reactive, smallrye-context-propagation, smallrye-health, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx]
----

==== Configure OpenShift Streams for Apache Kafka

Your OpenShift Streams for Apache Kafka instance requires some setup for use with OpenShift Connectors. This includes creating a *service account* to allow you to connect and authenticate your Connectors with the Kafka instance and setting up *access rules* for the service account to define how your Connectors can access and use the associated Kafka instance topics.

. Create a service account for your connector:
.. Navigate to the *Application and Data Services* page of the link:https://console.redhat.com[console.redhat.com] console.
.. In the Application and Data Services page on https://console.redhat.com[console.redhat.com], select *Service Accounts*, and then click *Create service account*.
.. Type a unique service account name (for example, `connectors` ) and then click *Create*.
.. Copy the generated *Client ID* and *Client Secret* to a secure location. You'll use these credentials to configure connections to your Streams for Apache Kafka instance.
+
[IMPORTANT]
====
The generated credentials are displayed only one time, so ensure that you've successfully and securely saved the copied credentials before closing the credentials window. 
====
.. Select the *I have copied the client ID and secret* option, and then click *Close*.

. Set the level of access for your new service account in the Access Control List (ACL) of the Kafka instance:
.. In the *Application and Data Services* page on https://console.redhat.com[console.redhat.com], select *Streams for Apache Kafka -> Kafka Instances*.
.. Click the name of your Streams for Apache Kafka instance.
.. Click the *Access* tab to view the current ACL for this instance.
.. Click *Manage access*.
.. From the *Account* drop-down menu, select the service account that you just created, and then click *Next*.
.. Under *Assign Permissions*, use the drop-down menus to set the permissions for this service account. +
Select the *Consume from a topic* and *Produce to a topic* from the *Task-based permission* possibilities. Set the topic and consumer group names to `is` and `*`.
+
image::images/rhosak-manage-access.png[]
+
Click *Save*.

Your Streams for Apache Kafka instance is now configured to be used with OpenShift Connectors.

==== Create an Openshift Connectors sink connector

OpenShift Connectors makes the distinction between source and sink connectors. A *source* connector allows you to send data from an external system to Streams for Apache Kafka. A *sink* connector allows you to send data from Streams for Apache Kafka to an external system.

In this workshop, you use the *HTTP Sink* connector which consumes Kafka messages from one or more topics and sends the messages to an HTTP endpoint.

The link:https://webhook.site[webhook.site, role=external,window=_blank] service offers a convenient way to obtain a general-purpose HTTP endpoint.

. In a browser window, navigate to link:https://webhook.site[role=external,window=_blank]. The page displays a unique URL that you can use as a data sink.
+
image::images/webhook-site-endpoint.png[]
+
Leave the browser window open. You will use it later in the lab to monitor the incoming requests.
. In a browser window, navigate to the *Application and Data Services* page on https://console.redhat.com[console.redhat.com], select *Connectors* and click *Create Connectors instance*.
. Select the connector that you want to use for a data sink.
+
You can browse through the catalog of available connectors. You can also search for a particular connector by name, and filter for sink or source connectors.
+
For example, to find the *HTTP* sink connector, type *HTTP* in the search box. The list filters to show only the *HTTP sink* connector card.
+
Click the card to select the connector, and click *Next*.
+
image::images/console-connectors-search-connector.png[]
. On the next screen, click the card for the Streams for Apache Kafka instance that you created before, and then click *Next*.
. On the *Namespace* page, click *Create preview namespace* to provision a namespace for hosting the connector instances that you create. This evaluation namespace will remain available for 48 hours. You can create up to four connector instances per namespace. Once the namespace is available, select it and click *Next*.
+
image::images/console-connectors-eval-namespace.png[]
. Provide the core configuration for your connector:
.. Type a unique name for the connector. e.g. `http-sink`.
.. Type the *Client ID* and *Client Secret* of the service account that you created for your connector and then click *Next*.
. Provide the connector-specific configuration for your connector. For the *HTTP Sink* connector, provide the following information:
.. *Data shape Format*: Accept the default, `application/octet-stream`.
.. *Method*: Accept the default, `POST`.
.. *URL*: Enter your unique URL from link:https://webhook.site[webhook.site, role=external,window=_blank].
.. *Topic Names*: Enter `globex.orders`. This is the name of the topic that receives the order payloads from the Order Placement service.
. Set the error handling policy to *stop*.
. Review the summary of the configuration properties and click *Create Connector*.

Your connector instance will be added to the table of connectors. After just a few seconds, the status of your connector instance will change to the `Ready` state.

==== Testing the Globex Coolstuff application

To test the end-to-end integration, you need to create one or more orders on the Globex Coolstuff website. The orders will be sent through REST to the Order Placement service, and from there pushed to a topic on the Streams for Apache Kafka broker. The Connector that you provisioned in the previous section consumes the messages from that topic and calls the link:https://webhook.site[webhook.site, role=external,window=_blank] HTTP endpoint with the message as payload.

. In a browser window, navigate to the console of the lab OpenShift cluster at %openshift_cluster_console%. Open the *Developer* perspective in the *globex-%user%* namespace.
. Open the *Globex UI* application in a browser tab. To do so, click on the image:images/console-open-url.png[] icon next to the blue circle representing the *Globex UI* deployment. Alternatively. open a tab browser and navigate to link:%globex_ui_url%[role=external,window=_blank]
. Click on *Login* in the upper right corner to simulate a login. In the Login pop-up window, enter an email address and password. The values don't really matter.
+
image::images/globex-coolstuff-login.png[]
. Click on *Coolstuff Store* to browse around the product catalog, and add some products to your shopping cart. Click on the *Cart* link to show the shopping cart.
+
image::images/globex-coolstuff-shopping-cart.png[]
. Click the *Proceed to Checkout* button to simulate a checkout. A new page opens showing a form for the order details. Click on the *Autofill* button to populate the form. Click *Submit Order*. 
. The order is sent to the Order Placement service, and from there to the `globex.orders` topic on the Streams for Apache Kafka broker. +
This can be verified using the message viewer functionality of Streams for Apache Kafka. +
Navigate to the *Application and Data Services -> Streams for Apache Kafka -> Kafka instances* page of link:https://console.redhat.com[console.redhat.com], select your Kafka instance and on the instance page select the *Topics* tab. Click on the `globex.orders` topic, and select the *Messages* tab. Notice the order message, with a JSON payload:
+
image::images/rhosak-messages-order.png[]
. The order message has been picked up by the HTTP sink connector you created previously, and used as payload to call the link:https://webhook.site[webhook.site, role=external,window=_blank] HTTP endpoint +
Open the browser tab pointing to link:https://webhook.site[webhook.site, role=external,window=_blank] to see the HTTP POST call with the order payload.
+
image::images/webhook-site-post-messages.png[]

Congratulations! You reached the end of this part of the workshop, in which you integrated the Globex Coolstuff application with an external cloud service, using the OpenShift Streams for Apache Kafka  and OpenShift Connectors managed cloud services. 
