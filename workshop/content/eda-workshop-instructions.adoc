:icons: font

== Adding Event Streaming capabilities to an application

=== Introduction

To support the business requirement of capturing and processing user activity on the Globex Coolstuff application, two new services have been developed:

* *Activity Tracking service*: This service exposes a REST endpoint. User activities on the Coolstuff website (such as opening a product page, liking a product etc..) generates an activity payload which is sent to the Activity Tracking REST endpoint. The service transforms this payload into a Kafka message which is sent to a topic on the Kafka broker.
* *Recommendation Engine*: This service consumes and processes the events produced by the Activity Tracking service. The service uses the Kafka Streams library to continuously determine the top featured products (the products which generate the most activities).
The service also exposes a REST endpoint to expose the list of featured products.

Both services are developed using Quarkus, the Quarkus extensions for reactive messaging and Kafka Streams. The development of the services is outside the scope of this workshop, but you are encouraged to examine the source code of the applications on GitHub: link:https://github.com/cloud-services-summit-connect-2022/activity-tracking-service[Activity Tracking Service,role=external,window=_blank] and link:https://github.com/cloud-services-summit-connect-2022/recommendation-engine[Recommendation Engine,role=external,window=_blank]

=== Workshop Activities

In this part of the workshop you will connect the Activity Tracking and Recommendation Engine applications to the OpenShift Streams for Apache Kafka instance using link:https://docs.openshift.com/container-platform/4.10/applications/connecting_applications_to_services/understanding-service-binding-operator.html[Service Binding,role=external,window=_blank].

You will be using the Red Hat Hybrid Cloud Console and the OpenShift Developer Console to setup and configure the Streams for Apache Kafka instance as well as the service binding. Note that these steps can also be achieved using the Red Hat OpenShift Application Services (`rhoas`) CLI.

.[underline]#Click to learn more about Service Binding#
[%collapsible]
====
Service Binding allows you to communicate connection details and secrets to an application to allow it to bind to a service. In this context, a service can be anything: a Kafka instance, a NoSQL database, etc. By using Service Binding, we no longer need to configure connection details (host, port), authentication mechanisms (SASL, OAuth) and credentials (username/password, client id/client secret) in an application. Instead, Service Binding injects these variables into your application container (as files or environment variables) for your application to consume. The Quarkus Kubernetes Service Binding extension enables Quarkus applications to automatically pickup these variables, injected as files, from the container's filesystem, removing the need to specify any configuration settings in the application resources (e.g configuration files) themselves.

https://servicebinding.io/[Service Binding For Kubernetes,role=external,window=_blank]. 
====

=== Step-By-Step Instructions

==== Explore the environment

. In a browser window, navigate to the console of the lab OpenShift cluster at link:%openshift_cluster_console%[role=external,window=_blank]. Login with your username and password (`%user%/%user_password%`). Open the *Developer* perspective in the *globex-%user%* namespace.
. In the Developer perspective, open the *Topology* view. Expect to see something like this (rearrange the topology as you see fit):
+
image::images/eda/globex-deployment-topology.png[]
+
The deployed topology consists of:
+
** `globex-ui`: The Globex Coolstuff web application (Node.js/Angular). 
** `catalog-app`: The Globex Coolstuff catalog service, consisting of the catalog database and the Spring Boot catalog microservice.
** `inventory-app`: The Globex Coolstuff inventory service, consisting of the inventory database and the Quarkus inventory microservice.
** `activity-tracking`: The Activity Tracking service. Notice that the deployment of the service is scaled to zero. The service will be scaled up once the connection to the Kafka broker is set up.
**  `recommendation-engine`: The Recommendation Engine service. Notice that the deployment of the service is scaled to zero. The service will be scaled up once the connection to the Kafka broker is set up.
** `order-placement-service`: The Order Placement service, which receives orders from the web application through REST and sends the orders to a Kafka topic. Notice that the deployment of the service is scaled to zero. The service will be scaled up once the connection to the Kafka broker is set up.
** `activity-tracking-simulator`: A Quarkus service that simulates user activity events and sends them to the Activity Tracking service.
. Open the *Globex UI* application in a browser tab. To do so, click on the image:images/eda/console-open-url.png[] icon next to the blue circle representing the *Globex UI* deployment. Alternatively. open a tab browser and navigate to link:%globex_ui_url%[role=external,window=_blank]
+
Expect to see the home page of the Globex Coolstuff web application:
+
image::images/eda/globex-coolstuff-home-page.png[]
. Click on *Cool Stuff Store* in the top menu to see a paginated list of products:
+
image::images/eda/globex-coolstuff-product-page.png[]
. The _Featured_ pane on the home page is empty at the moment. Also the product list page has an empty bar above the product list. These elements will be populated once the recommendation engine is up and running. 

==== Create a Kafka Topic in OpenShift Streams for Apache Kafka

. On the https://console.redhat.com[console.redhat.com] landing page, select *Application and Data Services* from the menu on the left. On the Application Services landing page, select *Streams for Apache Kafka → Kafka Instances*.

. In the *Kafka Instances* page of the web console, click the name of the Kafka instance (`globex`) that you want to add a topic to.

. Select the *Topics* tab, click *Create topic*, and follow the guided steps to define the topic details. Click *Next* to complete each step and click *Finish* to complete the setup.
+
* *Topic name*: Enter `globex.tracking`.
* *Partitions*: Keep the default value at `1`. 
* *Message retention*: Keep default values. [ *Retention time*: `A week` and *Retention Size*: `Unlimited`. ]
* *Replicas*: Keep default values
+
image::images/eda/rhosak-create-topic.png[]
+
.[underline]#*Click to learn more about these parameters*#
[%collapsible]
====
* Partitions are distinct lists of messages within a topic and enable parts of a topic to be distributed over multiple brokers in the cluster. A topic can contain one or more partitions, enabling producer and consumer loads to be scaled.
* Message retention time is the amount of time that messages are retained in a topic before they are deleted or compacted, depending on the cleanup policy. Retention size is the maximum total size of all log segments in a partition before they are deleted or compacted. For this workshop you can keep the default values.
* Replicas are copies of partitions in a topic. Partition replicas are distributed over multiple brokers in the cluster to ensure topic availability if a broker fails. When a follower replica is in sync with a partition leader, the follower replica can become the new partition leader if needed.
***For this release of Streams for Apache Kafka, the replicas are preconfigured. As the eval Kafka instance consists of only one broker, the number of partition replicas for the topic is set to `1`, as well as the minimum number of follower replicas that must be in sync with a partition leader. For a production Kafka broker on Streams for Apache Kafka these values will be `3` and `2` respectively. 
====

. After you complete the topic setup, the new Kafka topic is listed in the topics table. You can now start producing and consuming messages to and from this topic using services that are connected to this Kafka instance.
+
image::images/eda/rhosak-topic-created.png[]


==== Binding applications to Streams for Apache Kafka

Binding applications to services using Service Binding requires the Service Binding operator to be installed on the OpenShift cluster. To bind more specifically to a OpenShift Streams for Apache Kafka instance, the Red Hat OpenShift Application Services (RHOAS) operator is required. Both operators have been installed on your OpenShift cluster.

===== [underline]#*Connect OpenShift Streams for Apache Kafka*#

In this part of the workshop you connect your OpenShift instance to the Streams for Kafka instance you created previously. This can be done from the Developer perspective on the OpenShift console, or using the `rhoas` CLI. In this workshop you use the OpenShift console.

. In a browser window, navigate to the console of your OpenShift cluster at %openshift_cluster_console%. Open the *Developer* perspective in the *globex-%user%* namespace.
. In the Developer perspective, navigate to the *+Add* view. Locate the *Developer Catalog* card with the *Managed Services* entry.
+
image::images/eda/openshift-console-developer-catalog.png[]
. Click the *Managed Services* link. This opens the Managed Services page, which has a card for *Red Hat OpenShift Application Services*.
+
image::images/eda/openshift-console-application-services.png[]
. In order to discover the managed services you are entitled to, you need to unlock the functionality with a token obtained from link:https://console.redhat.com[console.redhat.com]. +
Open a new browser tab and navigate to link:https://console.redhat.com/openshift/token[console.redhat.com/openshift/token,role=external,window=_blank]. You should already be logged in, but if not, login with the Red Hat account ID you created or used earlier in the workshop. +
Click on *Load token* in the *Connect with offline token* box. Copy the generated API token.
. Go back to the browser tab with the OpenShift console, and click the *Red Hat OpenShift Application Services* card. This opens a popup window.  Paste the API token value in the *API Token* field.
+
image::images/eda/openshift-console-application-services-popup.png[]
+
Click *Connect*. 
+
[NOTE]
====
After clicking *Connect*, the connect button turns grey, but the *Red Hat OpenShift Application Services* window stays open until the discovery of the available managed services is complete. This can take up to a minute. Do not close the *Red Hat OpenShift Application Services* window or refresh your browser in the meantime, as this will interrupt the process and lead to errors afterwards.
====
+
Once the discovery is done, you are redirected back to the *Managed Services* page, which now shows a card for *Red Hat OpenShift Streams for Apache Kafka*.
+
image::images/eda/openshift-console-rhosak.png[]
. Click the *Red Hat OpenShift Streams for Apache Kafka* card, and click *Connect*. This opens a page which shows the Kafka instances that you can connect to. Select the `globex` instance and click *Next*
+
image::images/eda/openshift-console-rhosak-connect.png[]
. You are redirected to the *Topology View* of the Developer perspective, which shows now an icon for the managed Kafka instance (you might need to zoom out to see the icon if it was created off-screen).
+
image::images/eda/openshift-console-topology-rhosak.png[]
. The entry is backed by a `KafkaConnection` custom resource created by the OpenShift Application Services operator. To see the details of the KafkaConnection resource, click on the resource in the Topology view, and in the Details window, select *Edit KafkaConnection* from the *Action* drop-down box to see the YAML structure of the custom resource. +
Notice that the YAML structure contains the bootstrap URL of the Kafka broker, as well as a reference to a secret containing the details of a service account, named `rh-cloud-services-service-account`. More specifically, look for the `status.bootstrapServerHost` and `status.serviceAccountSecretName` fields in the YAML descriptor. +
Click *Cancel* to return to the Topology view.

===== [underline]#*Set Permissions for a Service Account*#

As part of connecting to the managed Kafka instance, a service account is created. This is the service account that will be used by the Activity Tracking and Recommendation Engine services to actually connect to the managed Kafka instance. To make this work, the service account needs permissions, in particular the service account needs to be able to consume from topics, produce to topics and create new topics.

Setting permissions in the Access Control List of a Streams for Apache Kafka can be done in the link:https://console.redhat.com[console.redhat.com] console, or using the `rhoas` CLI. In this workshop we use the UI on link:https://console.redhat.com[console.redhat.com].

. Navigate to the *Application and Data Services* page of the link:https://console.redhat.com[console.redhat.com] console.
. On the *Service Accounts* page, check that a service account was created by the OpenShift Application Services operator. Look for a service account with a name like `rhoas-operator-xxx`.
. Navigate to the *Streams for Apache Kafka -> Kafka instances* page and select your Kafka instance by clicking its name.
. Click the *Access* tab to view the current ACL for this instance.
+
image::images/eda/rhosak-default-access.png[]

. Click *Manage access*, use the *Account* drop-down menu to select the service account that was created by the OpenShift Application Services operator, and click *Next*.

. To set the permissions for this service account, under *Assign Permissions*, click the arrow of the *Add Permission* drop-down box. This opens a window with *Task-based permission*.
+
image::images/eda/rhosak-manage-access-add-permission.png[] 
+
Select the *Consume from a topic* and *Produce to a topic* from the *Task-based permission* possibilities. Set the topic and consumer group names to `is` and `*`.
+
image::images/eda/rhosak-manage-access.png[]
+
Click *Save*.
+
The ACL list for the service account should look like:
+
image::images/eda/rhosak-access-serviceaccount.png[]

===== [underline]#*Bind applications to Streams for Apache Kafka*#

You can now bind the Activity Tracking Service and Recommendation Engine to the OpenShift Streams for Apache Kafka instance. Through Service Binding the connection details are injected into the application pods. Service Binding to a managed Kafka instance can be done on the Topology view of OpenShift console, or through the `rhoas` CLI. In this workshop we use the OpenShift console.

. In a browser window, navigate to the console of your OpenShift cluster at %openshift_cluster_console%. Open the *Developer* perspective.
. Navigate to the *Topology* view of the OpenShift console in the *globex-%user%* namespace.
. Hover over the *activity-tracking* deployment, and grab the arrow that appears. Drag the arrow to the *KafkaConnection* icon. When reaching the KafkaConnection icon, a text box `Create Service Binding` appears. Release the arrow. Click *Create* in the *Create Service Binding* pop-up window. The Activity Tracking deployment and the KafkaConnection icon are now connected with a solid black arrow.
+
image::images/eda/rhosak-service-binding.png[]
. Click on the *activity-tracking* deployment to open the details window, and click on the deployment name (above the Details, Resources and Observe tabs) to open the full details of the Deployment. Scroll down to the *Volumes* section. Notice that the service binding occurs by injecting a secret into the pod:
+
image::images/eda/service-binding-secret.png[]
+
Go back to the Topology view.
. Scale the *activity-tracking* deployment to 1 replica. You can do so by clicking on the *activity-tracking* deployment, and in the details window select the *Details* tab, and click the arrow next to the circle to scale the deployment.
+
image::images/eda/openshift-console-scale-deployment.png[]
. Check the logs of the activity-tracking pod, and notice that the pod successfully connects to the Kafka broker instance. +
To see the logs, click the *Resources* tab of the deployment, and click on the *View logs* link. +
Expect to see something like: 
+
----
exec java -Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -XX:+ExitOnOutOfMemoryError -cp . -jar /deployments/quarkus-run.jar
__  ____  __  _____   ___  __ ____  ______ 
 --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ 
 -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\ \   
--\___\_\____/_/ |_/_/|_/_/|_|\____/___/   
2022-05-23 15:26:40,829 INFO  [org.apa.kaf.com.sec.aut.AbstractLogin] (main) Successfully logged in.
2022-05-23 15:26:41,061 INFO  [io.sma.rea.mes.kafka] (main) SRMSG18258: Kafka producer kafka-producer-tracking-event, connected to Kafka brokers 'globex-ca-m-q-mtp---qgalcrg.bf2.kafka.rhcloud.com:443', is configured to write records to 'globex.tracking'
2022-05-23 15:26:41,363 INFO  [io.quarkus] (main) activity-tracking-service 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.7.4.Final) started in 2.427s. Listening on: http://0.0.0.0:8080
2022-05-23 15:26:41,364 INFO  [io.quarkus] (main) Profile prod activated. 
2022-05-23 15:26:41,364 INFO  [io.quarkus] (main) Installed features: [cdi, kafka-client, resteasy-reactive, smallrye-context-propagation, smallrye-health, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx]
----
. Go back to the Topology view, and repeat the same steps to bind the *recommendation-engine* deployment to the OpenShift Streams for Apache instance. Once the service binding created, scale the deployment to 1 pod.
. Once the recommendation-engine is up and running, check in the link:https://console.redhat.com[console.redhat.com] console that a number of new topics have been created:
+
image::images/eda/rhosak-kafka-streams-topics.png[]
+
Those are the topics created by the Kafka Streams topology in the Recommendation Engine to calculate the top featured products based on activity events.

==== Testing the Globex Coolstuff application

Now that the Activity Tracking and Recommendation Engine apps are up and running, we can test the generation of activity events and the calculation of the top featured products.

The deployment topology for the workshop includes an activity simulator service which will generate a number of activity events randomly distributed over a list of products. These activity events are sent to the Activity Tracking service and transformed into Kafka messages in the `globex.tracking` topic. These messages are consumed by the Recommendation Engine app to calculate the top featured products.

. In a browser window, navigate to the console of the lab OpenShift cluster at %openshift_cluster_console%. Open the *Developer* perspective in the *globex-%user%* namespace.
. Open the URL to the *activity-tracking-simulator* application by clicking the image:images/eda/console-open-url.png[] icon next to the blue circle representing the *activity-tracking-simulator* deployment.
+
Alternatively. open a tab browser and navigate to link:%activity_tracking_simulator_url%/q/swagger-ui[role=external,window=_blank].
. Navigate to the `q/swagger-ui` path of the application. This opens a Swagger UI page which allows you to use the REST API of the application. The REST application has only one operation, `POST /simulate`.
+
image::images/eda/activity-tracking-simulator-swagger-ui.png[]
. Generate a number of activity events. Click the *Try it out* button, and set `count` to any value between 100 and 1000. Click *Execute*.
. OpenShift Streams for Apache Kafka has a message viewer functionality that allows you to inspect the contents of messages in a topic. +
Navigate to the to the *Application and Data Services -> Streams for Apache Kafka -> Kafka instances* page of link:https://console.redhat.com[console.redhat.com], select your Kafka instance and on the instance page select the *Topics* tab. Click on the `globex.tracking` topic, and select the *Messages* tab. Notice the activity event messages, with a JSON payload:
+
image::images/eda/rhosak-messages-tracking.png[]
. The featured product list calculated by the Recommendation Engine is produced to the `globex.recommendation-product-score-aggregated-changelog` topic. The list is recalculated roughly every 10 seconds as long as activity events are produced. Every calculation produces a message to the changelog topic. The last message in the topic represents the latest top featured list.
+
image::images/eda/rhosak-messages-aggregated-changelog.png[]
. In a browser window, navigate to the home page of the Globex Coolstuff web application. Notice that the home page now shows a list of featured products.
+
image::images/eda/globex-coolstuff-home-page-featured.png[]
+
Also, the product page now shows a banner with the featured products.
+
image::images/eda/globex-coolstuff-product-page-featured.png[]

Congratulations! You reached the end of this part of the workshop, in which you added event streaming capabilities to the Globex Coolstuff application, using the OpenShift Streams for Apache Kafka managed cloud service, and Service Binding to connect your apps to the Kafka instance. 


== Appendix: Use the `rhoas` CLI

If you prefer to use the `rhoas` CLI to provision and configure the OpenShift Streams for Apache Kafka instance, and to bind your applications to the Kafka instance using Service Binding, you can follow the following instructions:

* It is assumed that you have already installed the `rhoas` CLI. If not please do so by accessing the *Prerequsites Setup* page

* Login into Red Hat Application Services
+
[.console-input]
[source,bash]
----
rhoas login
----
+
This initiates a browser based login. Log in using your Red Hat Account credentials *you've created for this workshop*.

* Provision an evaluation Kafka instance:
** Provision the instance:
+
[.console-input]
[source,bash]
----
rhoas kafka create --name globex-%user% --region us-east-1
----
+
[.console-output]
[source,text]
----
{                                                                                                                  
  "cloud_provider": "aws",                                                                                         
  "created_at": "2022-05-23T17:20:03.700415552Z",                                                                  
  "href": "/api/kafkas_mgmt/v1/kafkas/ca5s4gjtq6jlcbnumh5g",                                                       
  "id": "ca5s4gjtq6jlcbnumh5g",                                                                                    
  "instance_type": "developer",                                                                                    
  "kafka_storage_size": "10Gi",                                                                                    
  "kind": "Kafka",                                                                                                 
  "multi_az": false,                                                                                               
  "name": "globex",                                                                                                
  "owner": "rh-bu-cloudservices-tmm",                                                                              
  "reauthentication_enabled": true,                                                                                
  "region": "us-east-1",                                                                                           
  "status": "accepted",                                                                                            
  "updated_at": "2022-05-23T17:20:03.700415552Z"                                                                   
}
----
** To check the status of the kafka instance:
+
[.console-input]
[source,bash]
----
rhoas status
----
+
[.console-output]
[source,text]
----
Service Context Name:   default
Context File Location:  /home/bernard/.config/rhoas/contexts.json

  Kafka
  -----------------------------------------------------------------------------
  ID:                     ca5s4gjtq6jlcbnumh5g
  Name:                   globex
  Status:                 ready
  Bootstrap URL:          globex-ca-s-gjtq-jlcbnumh-g.bf2.kafka.rhcloud.com:443
----

* Create a Kafka topic:
** Create the topic:
+
[.console-input]
[source,bash]
----
rhoas kafka topic create --name globex.tracking --partitions 1
----
** Verify the topics:
+
[.console-input]
[source,bash]
----
rhoas kafka topic list
----
+
[.console-output]
[source,text]
----
  NAME              PARTITIONS   RETENTION TIME (MS)   RETENTION SIZE (BYTES)  
 ----------------- ------------ --------------------- ------------------------ 
  globex.tracking            1   604800000             -1 (Unlimited)         
----

* Connect Streams for Apache Kafka instance.
** Before starting, make sure that you are connected to your OpenShift cluster using the `oc` CLI. 
** To connect your Kafka instance to your project, execute the following command in the terminal:
+
[.console-input]
[source,bash]
----
rhoas cluster connect -n globex-%user%
----
** You are asked to select the type of service you want to connect. Select *kafka* and press `enter`.
+
[.console-output]
[source,text]
----
? Select type of service  [Use arrows to move, type to filter]
> kafka
  service-registry
----
** The CLI will prints the *Connection Details* and asks you to confirm. Type `y` and press `enter` to continue.
+
[.console-output]
[source,text]
----
? Select type of service kafka
This command will link your cluster with Cloud Services by creating custom resources and secrets.
In case of problems please execute "rhoas cluster status" to check if your cluster is properly configured

Connection Details:

Service Type:                   kafka
Service Name:                   globex
Kubernetes Namespace:           globex
Service Account Secret:         rh-cloud-services-service-account

? Do you want to continue? (y/N) 
----
** You will be asked to provide a token, which can be retrieved from link:https://console.redhat.com/openshift/token[console.redhat.com/openshift/token]. Navigate to this URL, copy the token to your clipboard, and copy it into your terminal. Press `enter` to continue. 
+
You should see output similar to this:
+
[.console-output]
[source,text]
----
✔️  Token Secret "rh-cloud-services-accesstoken" created successfully
✔️  Service Account Secret "rh-cloud-services-service-account" created successfully

Client ID:     srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d

Make a copy of the client ID to store in a safe place. Credentials won't appear again after closing the terminal.

You will need to assign permissions to service account in order to use it.

You need to separately grant service account access to Kafka by issuing following command

  $ rhoas kafka acl grant-access --producer --consumer --service-account srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d --topic all --group all

✔️  kafka resource "globex" has been created
Waiting for status from kafka resource.
Created kafka can be already injected to your application.

To bind you need to have Service Binding Operator installed:
https://github.com/redhat-developer/service-binding-operator

You can bind kafka to your application by executing "rhoas cluster bind"
or directly in the OpenShift Console topology view.

✔️  Connection to service successful.
----
+
[NOTE]
====
The same command can also be run in a non-interactive way:

[.console-input]
[source,bash]
----
rhoas cluster connect -n globex-%user% --service-type kafka --service-name globex --token eyJhbGciOiJ...GDC-cTHCwgmxT-nzM -y
----
====

** To verify that the connection has been successfully created, execute the following `oc` command: 
+
[.console-input]
[source,bash]
----
oc get KafkaConnection -n globex-%user%
----
+
This should return a *KafkaConnection* with the name of your Kafka instance.
+
[.console-output]
[source,text]
----
NAME        AGE
globex      3m42s
----

* Assign permissions to the service account created by the OpenShift Application Services operator:
+
[.console-input]
[source,bash]
----
rhoas kafka acl grant-access --producer --consumer --service-account srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d --topic all --group all -y
----
+
[.console-output]
[source,text]
----
The following ACL rules will be created:

  PRINCIPAL (7)                                    PERMISSION   OPERATION   DESCRIPTION              
 ------------------------------------------------ ------------ ----------- ------------------------- 
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        describe    topic is "*"             
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        read        topic is "*"             
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        read        group is "*"             
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        write       topic is "*"             
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        create      topic is "*"             
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        write       transactional-id is "*"  
  srvc-acct-553dd8d3-e461-411d-a76c-7769bbb5c45d   allow        describe    transactional-id is "*"  

✔️  ACLs successfully created in the Kafka instance "globex"
----

* Bind an application to a Streams for Apache Kafka instance.
** Execute the following command:
+
[.console-input]
[source,bash]
----
rhoas cluster bind -n globex-%user%
----
** You are asked to select the application you want to connect to. Select *activity-tracking* and press `enter`. (When repeating for the second application, select *recommendation-engine*)
+
[.console-output]
[source,text]
----
Looking for Deployment resources. Use --deployment-config flag to look for deployment configs
? Please select application you want to connect with  [Use arrows to move, type to filter]
> activity-tracking
  activity-tracking-simulator
  catalog-database
  catalog-service
  globex-ui
  inventory-database
  inventory-service
  recommendation-engine
----
** You are asked to select the type of service you want to connect. Select *kafka* and press `enter`.
+
[.console-output]
[source,text]
----
Looking for Deployment resources. Use --deployment-config flag to look for deployment configs
? Please select application you want to connect with activity-tracking
? Select type of service  [Use arrows to move, type to filter]
> kafka
  service-registry
----
**  The CLI asks you to confirm. Type `y` and press `enter` to continue.
+
[.console-output]
[source,text]
----
Looking for Deployment resources. Use --deployment-config flag to look for deployment configs
? Please select application you want to connect with activity-tracking
? Select type of service kafka
Binding "globex" with "activity-tracking" app
? Do you want to continue? (y/N)
----
+
The CLI produces the following output:
+
[.console-output]
[source,text]
----
Using ServiceBinding Operator to perform binding
✔️  Binding globex with activity-tracking app succeeded
----
+
[NOTE]
====
The command can also be run in a non-interactive way:

[.console-input]
[source,bash]
----
rhoas cluster bind -n globex-%user% --app-name activity-tracking --service-type kafka --service-name globex -y
rhoas cluster bind -n globex-%user% --app-name recommendation-engine --service-type kafka --service-name globex -y
----
====